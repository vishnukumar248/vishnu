import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Bidirectional, GRU, Dense, MultiHeadAttention

def create_mhabigru_model(vocab_size, embedding_dim, gru_units, num_heads, output_dim):
  """Creates a Multi-Head Attention-based Bidirectional GRU model.

  Args:
    vocab_size: Vocabulary size.
    embedding_dim: Embedding dimension.
    gru_units: Number of GRU units.
    num_heads: Number of attention heads.
    output_dim: Output dimension (number of classes).

  Returns:
    A Keras model.
  """

  inputs = Input(shape=(None,))
  x = Embedding(vocab_size, embedding_dim)(inputs)

  # Bidirectional GRU layer
  x = Bidirectional(GRU(gru_units, return_sequences=True))(x)

  # Multi-Head Attention layer
  attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=gru_units)(x, x)

  # Concatenate the original input and attention output
  x = tf.concat([x, attention_output], axis=-1)

  # Final Dense layer for classification
  outputs = Dense(output_dim, activation='softmax')(x[:, -1, :])  # Use the last time step

  model = tf.keras.Model(inputs=inputs, outputs=outputs)
  return model